# Unified Configuration Example for Graphiti MCP Server
# Copy this file to config.local.yml and customize for your environment
# This file allows you to configure both LLM and embedder with different providers

# ============================================================================
# RECOMMENDED SETUP: Enterprise Gateway LLM + Local Ollama Embeddings
# ============================================================================
# This provides enterprise-grade LLM with fast, free local embeddings

llm:
  # LLM Configuration - Using OpenAI-compatible API (OpenAI/Bedrock/Azure)
  model: "gpt-4o"
  small_model: "gpt-4o-mini"  # Optional: smaller model for simpler tasks
  base_url: "https://api.openai.com/v1"  # Change to your enterprise gateway URL
  temperature: 0.1
  max_tokens: 8192

  # LLM-specific parameters (OpenAI/Bedrock)
  model_parameters:
    presence_penalty: 0.0
    frequency_penalty: 0.0
    top_p: 1.0
    n: 1
    stream: false

embedder:
  # Embedder Configuration - Using Local Ollama
  model: "nomic-embed-text"
  base_url: "http://localhost:11434/v1"
  dimension: 768

  # Embedder-specific parameters (Ollama)
  model_parameters:
    num_ctx: 4096

# ============================================================================
# ALTERNATIVE CONFIGURATIONS
# ============================================================================

# ----------------------------------------------------------------------------
# Option 1: All Ollama (Local Development - Free & Offline)
# ----------------------------------------------------------------------------
# llm:
#   model: "llama3.1:8b"
#   base_url: "http://localhost:11434/v1"
#   temperature: 0.1
#   max_tokens: 10000
#   model_parameters:
#     num_ctx: 4096
#     keep_alive: "5m"
#
# embedder:
#   model: "nomic-embed-text"
#   base_url: "http://localhost:11434/v1"
#   dimension: 768
#   model_parameters:
#     num_ctx: 4096

# ----------------------------------------------------------------------------
# Option 2: All Enterprise Gateway (Centralized Billing)
# ----------------------------------------------------------------------------
# llm:
#   model: "gpt-4o"
#   base_url: "https://your-enterprise-gateway.com"
#   temperature: 0.1
#   max_tokens: 8192
#
# embedder:
#   model: "text-embedding-3-small"
#   base_url: "https://your-enterprise-gateway.com/embeddings"
#   dimension: 1536

# ----------------------------------------------------------------------------
# Option 3: Azure OpenAI
# ----------------------------------------------------------------------------
# llm:
#   model: "gpt-4"  # Your Azure deployment model name
#   temperature: 0.1
#   max_tokens: 8192
#
# embedder:
#   model: "text-embedding-3-small"
#   dimension: 1536
#
# Required Azure environment variables:
#   AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com"
#   AZURE_OPENAI_API_VERSION="2024-02-01"
#   AZURE_OPENAI_DEPLOYMENT_NAME="your-deployment"
#   AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME="your-embedding-deployment"
#   OPENAI_API_KEY="your-azure-key"

# ============================================================================
# PROVIDER DETECTION (Automatic)
# ============================================================================
# The system automatically detects which provider to use based on base_url:
#   - localhost:11434 or 127.0.0.1:11434 -> Ollama
#   - azure.com in hostname -> Azure OpenAI
#   - Everything else -> OpenAI-compatible
#
# Each component (LLM and embedder) is detected independently, allowing
# mixed provider configurations.

# ============================================================================
# REQUIRED ENVIRONMENT VARIABLES
# ============================================================================
# Set these in your environment (not in this file):
#
# For OpenAI/Bedrock LLM (required if not using Ollama):
#   export OPENAI_API_KEY="your-api-key-here"
#
# For Neo4j (always required):
#   export NEO4J_URI="bolt://localhost:7687"
#   export NEO4J_USER="neo4j"
#   export NEO4J_PASSWORD="your-password"
#
# Optional SSL certificates (if your gateway requires them):
#   export SSL_CERT_FILE="/path/to/cert.pem"
#   export SSL_CA_BUNDLE="/path/to/ca-bundle.crt"

# ============================================================================
# TESTING YOUR CONFIGURATION
# ============================================================================
# After creating config.local.yml, test with:
#   uv run src/graphiti_mcp_server.py --transport stdio
#
# Check the startup logs for:
#   "Using [Provider] LLM: [model] at [url]"
#   "Using [Provider] embedder: [model] at [url]"

