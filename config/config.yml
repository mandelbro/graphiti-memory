# Unified Local Configuration for Graphiti MCP Server
# This file allows you to configure both LLM and embedder with different providers
# Each section can use a different provider - mix and match as needed!

llm:
  # LLM Configuration - Using Bedrock/Enterprise Gateway
  model: "gpt-5-mini"
  small_model: "gpt-5-nano"
  base_url: "https://api.openai.com/v1"
  temperature: 0.1
  max_tokens: 25000

  # LLM-specific parameters
  model_parameters:
    presence_penalty: 0.0
    frequency_penalty: 0.0
    top_p: 1.0
    n: 1
    stream: false

embedder:
  # Embedder Configuration - Using Local Ollama
  model: "nomic-embed-text"
  base_url: "http://localhost:11434/v1"
  dimension: 768

  # Embedder-specific parameters
  model_parameters:
    num_ctx: 4096

# Example configurations for different scenarios:
#
# Scenario 1: Both using OpenAI/Bedrock
# llm:
#   model: "gpt-4o"
#   base_url: "https://your-gateway.com"
# embedder:
#   model: "text-embedding-3-small"
#   base_url: "https://your-gateway.com/bedrock/embeddings"
#   dimension: 1536
#
# Scenario 2: Both using Ollama
# llm:
#   model: "llama3.1:8b"
#   base_url: "http://localhost:11434/v1"
# embedder:
#   model: "nomic-embed-text"
#   base_url: "http://localhost:11434/v1"
#   dimension: 768
#
# Scenario 3: Mixed (Current Configuration)
# llm:
#   model: "claude-sonnet-4-20250514"
#   base_url: "https://your-enterprise-gateway.com"
# embedder:
#   model: "nomic-embed-text"
#   base_url: "http://localhost:11434/v1"
#   dimension: 768
